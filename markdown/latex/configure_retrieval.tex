{[}\href{configure_indexing.html}{Previous: Configuring Indexing}{]}
{[}\href{index.html}{Contents}{]} {[}\href{learning.html}{Next: Learning
to Rank with Terrier}{]}

\section{Configuring Retrieval in
Terrier}\label{configuring-retrieval-in-terrier}

\subsection{Topics}\label{topics}

After the end of the indexing process, we can proceed with retrieving
from the document collection. At this stage, the configuration
properties for applying stemming or not, removing stopwords or not, and
the maximum length of terms, should be exactly the same properties as
used for indexing the collection.

Firstly, in the property \texttt{trec.topics}, we need to specify the
files containing the queries to process.

Moreover, before processing the queries, the tags of the topics files to
be processed should be specified. We can do that by setting the
properties \texttt{TrecQueryTags.process}, which denotes which tags to
process, \texttt{TrecQueryTags.idtag}, which stands for the tag
containing the query identifier, and \texttt{TrecQueryTags.skip}, which
denotes which query tags to ignore.

For example, suppose that the format of topics is the following:

\begin{verbatim}
<TOP>
<NUM>123<NUM>
<TITLE>title
<DESC>description
<NARR>narrative
</TOP>
\end{verbatim}

If we want to skip the description and narrative (DESC and NARR tags
respectively), and consequently use the title only, then we need to
setup the properties as follows:

\begin{verbatim}
TrecQueryTags.doctag=TOP
TrecQueryTags.process=TOP,NUM,TITLE
TrecQueryTags.idtag=NUM
TrecQueryTags.skip=DESC,NARR
\end{verbatim}

If alternatively, we want to use the title, description and the
narrative tags to create the query, then we need to setup the properties
as follows:

\begin{verbatim}
TrecQueryTags.doctag=TOP
TrecQueryTags.process=TOP,NUM,DESC,NARR,TITLE
TrecQueryTags.idtag=NUM
TrecQueryTags.skip=
\end{verbatim}

The tags specified by TrecQueryTags are case-insensitive (note the
difference from TrecDocTags). If you want them to be case-sensitive,
then set \texttt{TrecQueryTags.casesensitive=false}.

\subsection{Weighting Models and
Parameters}\label{weighting-models-and-parameters}

Next, we need to specify which of the available weighting models we will
use for assigning scores to the retrieved documents. We do this by
specifying the name of the corresponding model class in the property
\texttt{trec.model}. E.g. \texttt{trec.model=PL2}.

Terrier provides implementations of many weighting models (see
\href{javadoc/org/terrier/matching/models/package-summary.html}{org.terrier.matching.models}
for the full list). In particular, some of the notable weighting models
implemented include many from the \href{dfr_description.html}{Divergence
from Randomness (DFR) framework}, among others:

\begin{itemize}
\tightlist
\item
  \href{javadoc/org/terrier/matching/models/BB2.html}{BB2} (DFR):
  Bose-Einstein model for randomness, the ratio of two Bernoulli's
  processes for first normalisation, and Normalisation 2 for term
  frequency normalisation \protect\hyperlink{cite1}{{[}1{]}}.
\item
  \href{javadoc/org/terrier/matching/models/BM25.html}{BM25}: The BM25
  probabilistic model.
\item
  \href{javadoc/org/terrier/matching/models/DFR_BM25.html}{DFR\_BM25}
  (DFR): The DFR version of BM25 \protect\hyperlink{cite1}{{[}1{]}}.
\item
  \href{javadoc/org/terrier/matching/models/DLH.html}{DLH} (DFR): The
  DLH hyper-geometric DFR model (parameter free).
\item
  \href{javadoc/org/terrier/matching/models/DLH13.html}{DLH13} (DFR): An
  improved version of DLH (parameter free).
\item
  \href{javadoc/org/terrier/matching/models/DPH.html}{DPH} (DFR): A
  different hyper-geometric DFR model using Popper's normalization
  (parameter free) \protect\hyperlink{cite2}{{[}2{]}}.
\item
  \href{javadoc/org/terrier/matching/models/DFRee.html}{DFRee} (DFR):
  Another hyper-geometric models which takes an average of two
  information measures.
\item
  \href{javadoc/org/terrier/matching/models/Hiemstra_LM.html}{Hiemstra\_LM}:
  Hiemstra's language model.
\item
  \href{javadoc/org/terrier/matching/models/IFB2.html}{IFB2} (DFR):
  Inverse Term Frequency model for randomness, the ratio of two
  Bernoulli's processes for first normalisation, and Normalisation 2 for
  term frequency normalisation \protect\hyperlink{cite1}{{[}1{]}}.
\item
  \href{javadoc/org/terrier/matching/models/In_expB2.html}{In\_expB2}
  (DFR): Inverse expected document frequency model for randomness, the
  ratio of two Bernoulli's processes for first normalisation, and
  Normalisation 2 for term frequency normalisation
  \protect\hyperlink{cite1}{{[}1{]}}.
\item
  \href{javadoc/org/terrier/matching/models/In_expC2.html}{In\_expC2}
  (DFR): Inverse expected document frequency model for randomness, the
  ratio of two Bernoulli's processes for first normalisation, and
  Normalisation 2 for term frequency normalisation with natural
  logarithm \protect\hyperlink{cite1}{{[}1{]}}.
\item
  \href{javadoc/org/terrier/matching/models/InL2.html}{InL2} (DFR):
  Inverse document frequency model for randomness, Laplace succession
  for first normalisation, and Normalisation 2 for term frequency
  normalisation \protect\hyperlink{cite1}{{[}1{]}}.
\item
  \href{javadoc/org/terrier/matching/models/LemurTF_IDF.html}{LemurTF\_IDF}:
  Lemur's version of the tf*idf weighting function.
\item
  \href{javadoc/org/terrier/matching/models/LGD.html}{LGD} (DFR): A
  log-logistic DFR model \protect\hyperlink{cite3}{{[}3{]}},
  \protect\hyperlink{cite4}{{[}4{]}}.
\item
  \href{javadoc/org/terrier/matching/models/PL2.html}{PL2} (DFR):
  Poisson estimation for randomness, Laplace succession for first
  normalisation, and Normalisation 2 for term frequency normalisation
  \protect\hyperlink{cite1}{{[}1{]}}.
\item
  \href{javadoc/org/terrier/matching/models/TF_IDF.html}{TF\_IDF}: The
  tf*idf weighting function, where tf is given by Robertson's tf and idf
  is given by the standard Sparck Jones' idf.
\item
  \href{javadoc/org/terrier/matching/models/DFRWeightingModel.html}{DFRWeightingModel}:
  This class provides an alternative way of specifying an arbitrary DFR
  weighting model, by mixing the used components
  \protect\hyperlink{cite1}{{[}1{]}}. For usage, see
  \href{extend_retrieval.html}{Extending Retrieval} and background
  material in \href{dfr_description.html}{Description of DFR}.
\end{itemize}

To process the queries, ensure the topics are specified in the
\texttt{trec.topics} property, then type the following:

\begin{verbatim}
bin/trec_terrier.sh -r -c 1.0 
\end{verbatim}

where the option \texttt{-r} specifies that we want to perform
retrieval, and the option \texttt{-c\ 1.0} specifies the parameter value
for the term frequency normalisation.

To process queries using a specific weighting model, we can
\emph{override} the \texttt{trec.model} property on the command line:

\begin{verbatim}
bin/trec_terrier.sh -r -Dtrec.model=DLH13
\end{verbatim}

-D tells TrecTerrier that we wish to override a property.

\href{}{}

\subsection{Field-Based Weighting
Models}\label{field-based-weighting-models}

Starting with version 3.0, Terrier has support for field-based weighting
models. In particular, field-based models take into account not just the
presence of a term in a field, but the actual frequency of the
occurrence in that field. For instance, for a document where the query
term occurs once in the body of the text, then there is only a small
chance that the document is really related to that term. However, if the
term occurs in the title of the document, then this chance is greatly
increased. Terrier provides several field-based weighting models:

\begin{itemize}
\tightlist
\item
  \href{javadoc/org/terrier/matching/models/PL2F.html}{PL2F}: this is a
  per-field normalisation model, which is based on PL2
  \protect\hyperlink{cite7}{{[}7{]}}.
\item
  \href{javadoc/org/terrier/matching/models/BM25F.html}{BM25F}: this is
  a per-field normalisation model, which is based on BM25.
\item
  \href{javadoc/org/terrier/matching/models/ML2.html}{ML2}: this is
  multinomial field-based model \protect\hyperlink{cite8}{{[}8{]}}.
\item
  \href{javadoc/org/terrier/matching/models/MDL2.html}{MDL2}: this is
  another multinomial field-based model, where the multinomial is
  replaced by an approximation \protect\hyperlink{cite8}{{[}8{]}}.
\item
  Arbitrary per-field normalisation weighting models can be generated
  using
  \href{javadoc/org/terrier/matching/models/PerFieldNormWeightingModel.html}{PerFieldNormWeightingModel}
  in a similar manner to DFRWeightingModel.
\end{itemize}

To use a field-based model, you have to index using fields. See
\href{configure_indexing.html}{Configuring Indexing} for more details on
how to configure fields during indexing.

Different field-based models have different parameters, as controlled by
various properties. These generally include weights for each field,
namely \texttt{w.0}, \texttt{w.1}, etc. Per-field normalisation models,
such as BM25F and PL2F also require the normalisation parameters for
each field, namely \texttt{c.0}, \texttt{c.1}, and so on. To run with a
field-based model:

\begin{verbatim}
bin/trec_terrier.sh -r -Dtrec.model=PL2F -Dc.0=1.0 -Dc.1=2.3 -Dc.3=40 -Dw.0=4 -Dw.1=2 -Dw.3=25
\end{verbatim}

For improved efficiency of field-based weighting models, it is
recommended that you manually alter the \texttt{data.properties} file of
your index to change the DocumentIndex implementation in use, by
updating it to read
\texttt{index.document.class=org.terrier.structures.FSAFieldDocumentIndex}.
\href{}{}

\subsection{Proximity (Dependence)
Models}\label{proximity-dependence-models}

Starting with version 3.0, Terrier includes two dependence models. Such
models highly weight documents where the query terms are in close
proximity. To use a term dependence model, you have to index using
blocks - see \href{configure_indexing.html}{Configuring Indexing} for
more details on how to configure block indexing.

Two dependence models are included:

\begin{itemize}
\tightlist
\item
  \href{javadoc/org/terrier/matching/dsms/DFRDependenceScoreModifier.html}{DFRDependenceScoreModifier}
  - this implements a Divergence from Randomness based dependence model
  \protect\hyperlink{cite6}{{[}6{]}}.
\item
  \href{javadoc/org/terrier/matching/dsms/MRFDependenceScoreModifier.html}{MRFDependenceScoreModifier}
  - this implements the Markov Random Field dependence model
  \protect\hyperlink{cite5}{{[}5{]}}.
\end{itemize}

To enable the dependence models, use the \texttt{matching.dsms}
property. E.g. :

\begin{verbatim}
bin/trec_terrier.sh -r -Dmatching.dsms=DFRDependenceScoreModifier
\end{verbatim}

The dependence models have various parameters to set. For more
information, see the classes themselves.

\href{}{}

\subsection{Document Prior Features}\label{document-prior-features}

Terrier can easily integrate a query-independent document feature (or
prior) into your retrieval model. The simplest way to do this is using
\href{javadoc/org/terrier/matching/dsms/SimpleStaticScoreModifier.html}{SimpleStaticScoreModifier}.
For instance, say you generate a feature for all documents in the
collection (e.g. using link analysis). You should export your file in
one of the formats supported by SimpleStaticScoreModifier, e.g. feature
value for each document, one per line. You can then add the feature as:

\begin{verbatim}
bin/trec_terrier.sh -r -Dmatching.dsms=SimpleStaticScoreModifier -Dssa.input.file=/path/to/feature -Dssa.input.type=listofscores -Dssa.w=0.5
\end{verbatim}

The property \texttt{ssa.w} controls the weight of your feature. For
more information on the type of files supported, see
\href{javadoc/org/terrier/matching/dsms/SimpleStaticScoreModifier.html}{SimpleStaticScoreModifier}.
Finally, Terrier can support multiple DSMs, using them in a
comma-delimited manner:

\begin{verbatim}
bin/trec_terrier.sh -r -Dmatching.dsms=DFRDependenceScoreModifier,SimpleStaticScoreModifier -Dssa.input.file=/path/to/feature -Dssa.input.type=listofscores -Dssa.w=0.5
\end{verbatim}

\href{}{}

\subsection{Query Expansion}\label{query-expansion}

Terrier also offers a query expansion functionality. For a brief
description of the query expansion module, you may view the
\href{dfr_description.html\#queryexpansion}{query expansion section of
the DFR Framework description}. The term weighting model used for
expanding the queries with the most informative terms of the top-ranked
documents is specified by the property \texttt{trec.qe.model}, the
default value is
\href{javadoc/org/terrier/matching/models/queryexpansion/Bo1.html}{Bo1},
which refers to the class implemnting the term weighting model to be
used for query expansion. Terrier has other query expansion models,
including
\href{javadoc/org/terrier/matching/models/queryexpansion/Bo2.html}{Bo2}
and
\href{javadoc/org/terrier/matching/models/queryexpansion/KL.html}{KL} -
see
\href{javadoc/org/terrier/matching/models/queryexpansion/package-summary.html}{org.terrier.matching.models.queryexpansion}
for the full list.

In addition, there are two parameters that can be set for applying query
expansion. The first one is the number of terms to expand a query with,
specified by the property \texttt{expansion.terms} - default value
\texttt{10}. Moreover, the number of top-ranked documents from which
these terms are extracted is specified by the property
\texttt{expansion.documents}, the default value of which is 3.

To retrieve from an indexed test collection, using query expansion, with
the term frequency normalisation parameter equal to 1.0, we can type:

\begin{verbatim}
bin/trec_terrier.sh -r -q -c 1.0 
\end{verbatim}

Relevance feedback is also supported by Terrier, assuming that the
relevant documents are listed in a TREC format ``qrels'' file. To use
feedback documents in query expansion, change the
\href{javadoc/org/terrier/querying/FeedbackSelector.html}{FeedbackSelector},
as follows:

\begin{verbatim}
bin/trec_terrier.sh -r -q -Dqe.feedback.selector=RelevantOnlyFeedbackDocuments,RelevanceFeedbackSelector -Dqe.feedback.filename=/path/to/feedback/qrels
\end{verbatim}

\href{}{}

\subsection{Learning to Rank}\label{learning-to-rank}

Since version 4.0, Terrier has offered learning to rank functionality,
based on the so-called Fat framework. This allows multiple features
(which can be query-dependent or query-independent) to be calculated
during the Matching process, and then combined using a machine learned
ranking model. In particular, any weighting model in Terrier can be used
as an additional query-dependent feature. For more information on these
new, advanced functionalities in Terrier - including a worked example
using a TREC Web track corpus - see \href{learning.html}{Learning to
Rank with Terrier}.

\subsection{Other Configurables}\label{other-configurables}

The results are saved in the directory var/results in a file named as
follows:

\begin{verbatim}
"weighting scheme" c "value of c"_counter.res
\end{verbatim}

For example, if we have used the weighting scheme PL2 with c=1.28 and
the counter was 2, then the filename of the results would be
\texttt{PL2c1.28\_3.res}. If you wish to override the filename of the
generated result file, use the \texttt{trec.results.file} property.
Alternatively, if multiple instances of Terrier are writing files at
same time, the use of the counter can fail due to a race condition.
Instead, set \texttt{trec.querycounter.type=random}. Output files by
TRECQuerying are always in the TREC-format. If you desire an alternative
format, you can implement another
\href{javadoc/org/terrier/structures/outputformat/OutputFormat.html}{org.terrier.structures.outputformat.OutputFormat},
then get TRECQuerying to use this with the property
\texttt{trec.querying.outputformat}.

For each query, Terrier returns a maximum number of 1000 documents by
default. We can change the maximum number of returned documents per
query by changing \texttt{matching.retrieved\_set\_size}. For example,
if we want to retrieve 10000 documents for each given query, we need to
set \texttt{matching.retrieved\_set\_size} to 10000. In addition, if the
\texttt{end} control is set in the property
\texttt{querying.default.controls}, then amend this to 9999 as well
(from Terrier 3.5, this is removed from the default configuration).
TRECQuerying can also limit this number, according to the
\texttt{trec.output.format.length} property (default 1000) also.

Some of the weighting models, e.g. BM25, assume low document frequencies
of query terms. For these models, it is worth ignoring query terms with
high document frequency during retrieval by setting
\texttt{ignore.low.idf.terms} to true. Moreover, it is better to set
\texttt{ignore.low.idf.terms} to false for high precision search tasks
such as named-page finding.

\subsection{Bibliography}\label{bibliography}

\begin{enumerate}
\tightlist
\item
  \href{}{}Probabilistic Models for Information Retrieval based on
  Divergence from Randomness. G. Amati. PhD Thesis, School of Computing
  Science, University of Glasgow, 2003.
\item
  \href{}{}FUB, IASI-CNR and University of Tor Vergata at TREC 2007 Blog
  Track. G. Amati and E. Ambrosi and M. Bianchi and C. Gaibisso and G.
  Gambosi. Proceedings of the 16th Text REtrieval Conference
  (TREC-2007), 2008.
\item
  \href{}{}Bridging Language Modeling and Divergence From Randomness
  Approaches: A Log-logistic Model for IR. Stephane Clinchant and Eric
  Gaussier. In Proceedings of ICTIR 2009, London, UK.
\item
  \href{}{}Information-Based Models for Ad Hoc Information Retrieval. S.
  Clinchant and E. Gaussier. In Proceedings of SIGIR 2010, Geneva,
  Switzerland.
\item
  \href{}{}A Markov Random Field Model for Term Dependencies. D. Metzler
  and W.B. Croft. Proceedings of the 28th annual international ACM SIGIR
  conference on Research and development in information retrieval (SIGIR
  2005), 472-479, Salvador, Brazil, 2005.
\item
  \href{}{}Incorporating Term Dependency in the DFR Framework. J. Peng,
  C. Macdonald, B. He, V. Plachouras and I. Ounis.
\item
  \href{}{}University of Glasgow at WebCLEF 2005: Experiments in
  per-field normalisation and language specific stemming. C. Macdonald,
  V. Plachouras, B. He, C. Lioma and I. Ounis. In Working notes of the
  CLEF 2005 Workshop, Vienna, Austria, 2005.
\item
  \href{}{}Multinomial Randomness Models for Retrieval with Document
  Fields. V. Plachouras and I. Ounis. Proceedings of the 29th European
  Conference on Information Retrieval (ECIR07). Rome, Italy, 2007.
\end{enumerate}

{[}\href{configure_indexing.html}{Previous: Configuring Indexing}{]}
{[}\href{index.html}{Contents}{]} {[}\href{learning.html}{Previous:
Learning to Rank with Terrier}{]}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Webpage: \url{http://terrier.org}\\
Contact:
\href{mailto:terrier@dcs.gla.ac.uk}{\nolinkurl{terrier@dcs.gla.ac.uk}}\\
\href{http://www.dcs.gla.ac.uk/}{School of Computing Science}\\
Copyright (C) 2004-2015 \href{http://www.gla.ac.uk/}{University of
Glasgow}. All Rights Reserved.
